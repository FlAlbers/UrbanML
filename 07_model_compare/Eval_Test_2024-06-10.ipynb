{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "538fe83c",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [6]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65f1087d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T20:15:17.018686Z",
     "iopub.status.busy": "2024-06-10T20:15:17.018686Z",
     "iopub.status.idle": "2024-06-10T20:15:21.069398Z",
     "shell.execute_reply": "2024-06-10T20:15:21.069398Z"
    },
    "papermill": {
     "duration": 4.060713,
     "end_time": "2024-06-10T20:15:21.071397",
     "exception": false,
     "start_time": "2024-06-10T20:15:17.010684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Relevant Libraries, the model and the test data\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "from tensorflow.keras import utils\n",
    "import joblib\n",
    "import pickle\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "import seaborn as sns\n",
    "from modules.sequence_and_normalize import sequence_data, sequence_sample_random, sequence_list\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from modules.plots import plot_seq_i_d_Q\n",
    "from modules.save_load_model import load_model, load_model_container\n",
    "from modules.predict_and_prepare import pred_inverse_all, pred_and_add_durIndex, pred_all_list\n",
    "from modules.eval_helpers import rmse_from_raw, mae_from_raw, mae_mse_rmse\n",
    "from IPython.display import display, Markdown, Latex, Image\n",
    "import warnings\n",
    "import matplotlib.patches as mpatches\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d96301b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T20:15:21.082523Z",
     "iopub.status.busy": "2024-06-10T20:15:21.082523Z",
     "iopub.status.idle": "2024-06-10T20:15:21.086060Z",
     "shell.execute_reply": "2024-06-10T20:15:21.086060Z"
    },
    "papermill": {
     "duration": 0.011462,
     "end_time": "2024-06-10T20:15:21.087989",
     "exception": false,
     "start_time": "2024-06-10T20:15:21.076527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_names = ['Gievenbeck_LSTM_Single_MSE_u128_2024-05-09']\n",
    "model_alias = ['\"W1\"']\n",
    "base_folder = os.path.join('05_models', 'test_wehr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7912f6cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T20:15:21.098848Z",
     "iopub.status.busy": "2024-06-10T20:15:21.098848Z",
     "iopub.status.idle": "2024-06-10T20:15:21.102673Z",
     "shell.execute_reply": "2024-06-10T20:15:21.102673Z"
    },
    "papermill": {
     "duration": 0.010681,
     "end_time": "2024-06-10T20:15:21.103671",
     "exception": false,
     "start_time": "2024-06-10T20:15:21.092990",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Notebook parameters\n",
    "model_names = ['Gievenbeck_LSTM_Single_MSE2024-05-16', 'Gievenbeck_LSTM_Single_MAE2024-05-16']\n",
    "model_alias = ['\"Loss MSE\"','\"Loss MAE\"']\n",
    "base_folder = os.path.join('05_models', 'loss_functions_compare')\n",
    "# model_names = ['Gievenbeck_LSTM_Single_MSE_u128_2024-05-03']\n",
    "# model_alias = ['\"Units = 128\"']\n",
    "# base_folder = os.path.join('05_models', 'units_compare')\n",
    "title = 'Vergleich Verlustfunktionen'\n",
    "\n",
    "\n",
    "# export_name = 'Test_RKB' + '_' + str(date.today()) + '.ipynb'\n",
    "model_names = ['Gievenbeck_RKB_LSTM_2024-05-30']\n",
    "model_alias = ['\"RKB BÜ\"']\n",
    "base_folder = os.path.join('05_models', 'test_RKB')\n",
    "title = 'Test RKB'\n",
    "\n",
    "# model_names = ['Gievenbeck_LSTM_Single_MAE_GPU2024-05-19', 'Gievenbeck_LSTM_Triple_MSE_u128_GPU_2024-05-19']\n",
    "# model_alias = ['\"Initial GPU\"','\"Final GPU\"']\n",
    "# base_folder = os.path.join('05_models', 'final_compare')\n",
    "# title = 'Vergleich des Finalen Modells mit Grafikkarte'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c4197d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T20:15:21.114537Z",
     "iopub.status.busy": "2024-06-10T20:15:21.114537Z",
     "iopub.status.idle": "2024-06-10T20:15:21.117505Z",
     "shell.execute_reply": "2024-06-10T20:15:21.117505Z"
    },
    "papermill": {
     "duration": 0.009964,
     "end_time": "2024-06-10T20:15:21.118502",
     "exception": false,
     "start_time": "2024-06-10T20:15:21.108538",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "model_names = [\"Gievenbeck_LSTM_Test\"]\n",
    "model_alias = [\"\\\"Test Model\\\"\"]\n",
    "base_folder = \"05_models\\\\train_test\"\n",
    "title = \"Test evaluation\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62e3018b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T20:15:21.129388Z",
     "iopub.status.busy": "2024-06-10T20:15:21.129388Z",
     "iopub.status.idle": "2024-06-10T20:15:21.137070Z",
     "shell.execute_reply": "2024-06-10T20:15:21.137070Z"
    },
    "papermill": {
     "duration": 0.014566,
     "end_time": "2024-06-10T20:15:21.138068",
     "exception": false,
     "start_time": "2024-06-10T20:15:21.123502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Test evaluation"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erstellt am: 2024-06-10 22:15\n"
     ]
    }
   ],
   "source": [
    "display(Markdown('# ' + title))\n",
    "import datetime\n",
    "\n",
    "current_time = datetime.datetime.now()\n",
    "current_time_minutes = current_time.strftime(\"%H:%M\")\n",
    "\n",
    "\n",
    "current_date = datetime.date.today()\n",
    "print(\"Erstellt am:\", current_date, current_time_minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d30198",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfb7760f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T20:15:21.148980Z",
     "iopub.status.busy": "2024-06-10T20:15:21.148980Z",
     "iopub.status.idle": "2024-06-10T20:15:22.403140Z",
     "shell.execute_reply": "2024-06-10T20:15:22.403140Z"
    },
    "papermill": {
     "duration": 1.26107,
     "end_time": "2024-06-10T20:15:22.404138",
     "exception": true,
     "start_time": "2024-06-10T20:15:21.143068",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Das System kann den angegebenen Pfad nicht finden: '05_models\\\\train_test\\\\Gievenbeck_LSTM_Test'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m model_names:\n\u001b[0;32m      4\u001b[0m     model_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_folder, model_name)\n\u001b[1;32m----> 5\u001b[0m     model_container \u001b[38;5;241m=\u001b[39m load_model_container(model_folder, print_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m     model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(model_container[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselect_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# model = model_container[model_id]\u001b[39;00m\n",
      "File \u001b[1;32m~\\PythonProjects\\urbanml\\modules\\save_load_model.py:293\u001b[0m, in \u001b[0;36mload_model_container\u001b[1;34m(save_folder, print_info)\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded model from disk\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    292\u001b[0m i\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 293\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(save_folder):\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    295\u001b[0m         i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Das System kann den angegebenen Pfad nicht finden: '05_models\\\\train_test\\\\Gievenbeck_LSTM_Test'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get Model and Data\n",
    "models = []\n",
    "for model_name in model_names:\n",
    "    model_folder = os.path.join(base_folder, model_name)\n",
    "    model_container = load_model_container(model_folder, print_info=False)\n",
    "    model_id = 'model_' + str(model_container['select_id'])\n",
    "    # model = model_container[model_id]\n",
    "    model = model_container['selected_model']\n",
    "    model['cv_scores'] = model_container['cv_scores']\n",
    "    comb_history = {}\n",
    "    for key in model_container['selected_model']['history'].keys():\n",
    "        sel_history = model_container['selected_model']['history'][key]\n",
    "        prev_history = model_container[model_id]['history'][key]\n",
    "        comb_history[key] = prev_history + sel_history\n",
    "    model['combined_history'] = comb_history\n",
    "\n",
    "    models.append(model)\n",
    "\n",
    "out_unit = 'm³/s'\n",
    "\n",
    "metrics_labels = { 'mape': 'Mittlerer absoluter prozentualer Fehler - MAPE [%]',\n",
    "                  'mae': 'Mittlerer absoluter Fehler - MAE [' + out_unit + ']', \n",
    "                  'rmse': 'Wurzel der mittleren Fehlerquadratsumme - RMSE [' + out_unit + ']',\n",
    "                   'mse': 'Mittlere Fehlerquadratsumme - MSE [-]',}\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fbe013",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if test_data is same\n",
    "sum_check = []\n",
    "for m in models:\n",
    "    sum_m = 0\n",
    "    for event in m['train_data']:\n",
    "        sum_m = sum_m + sum(event[1]['duration'])\n",
    "\n",
    "    sum_check.append(sum_m)\n",
    "\n",
    "test_same = all(x == sum_check[0] for x in sum_check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8096394",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict and prepare all data for all given models\n",
    "\n",
    "for m in models:\n",
    "    model = m['model']\n",
    "    lag = m['lag']\n",
    "    delay = m['delay']\n",
    "    p_steps = m['prediction_steps']\n",
    "    in_vars_future = m['in_vars']\n",
    "    try:\n",
    "        in_vars_past = m['in_vars_past']\n",
    "    except:\n",
    "        in_vars_past = None\n",
    "        pass\n",
    "    out_vars = m['out_vars']\n",
    "    if test_same == True:\n",
    "        test_data = m['test_data']\n",
    "    else:\n",
    "        test_data = models[1]['test_data']\n",
    "    train_data = m['train_data']\n",
    "    # val_data = m['validation_data']\n",
    "    in_scaler = m['in_scaler']\n",
    "    out_scaler = m['out_scaler']\n",
    "\n",
    "    seq_train, seq_train_trans = sequence_list(train_data, in_vars_future=in_vars_future, out_vars=out_vars, in_scaler=in_scaler, \n",
    "                                    out_scaler=out_scaler, lag=lag, delay=delay, prediction_steps=p_steps, in_vars_past=in_vars_past)\n",
    "    \n",
    "    # seq_val, seq_val_trans = sequence_list(val_data, in_vars=in_vars, out_vars=out_vars, in_scaler=in_scaler, \n",
    "                                    # out_scaler=out_scaler, lag=lag, delay=delay, prediction_steps=p_steps)\n",
    "    \n",
    "    seq_test, seq_test_trans = sequence_list(test_data, in_vars_future=in_vars_future, out_vars=out_vars, in_scaler=in_scaler, \n",
    "                                    out_scaler=out_scaler, lag=lag, delay=delay, prediction_steps=p_steps, in_vars_past=in_vars_past)\n",
    "\n",
    "    # print(seq_test[0][2])\n",
    "    # print(seq_test[0][2])\n",
    "    # print(test_data[0][1])\n",
    "    # Strukture:    pred_list[event_id][data_id][sequence_id] where data_id = 0 is event meta data, \n",
    "    #               data_id = 1 is input data, data_id = 2 is true data and data_id = 3 is predicted data\n",
    "    test_pred_list = pred_and_add_durIndex(model, out_scaler, seq_test, seq_test_trans)\n",
    "\n",
    "    train_true, train_pred = pred_inverse_all(train_data, model, in_vars_future, out_vars, in_scaler, out_scaler, lag, delay, p_steps, in_vars_past=in_vars_past)\n",
    "    # val_true, val_pred = pred_inverse_all(val_data, model, in_vars, out_vars, in_scaler, out_scaler, lag, delay, p_steps)\n",
    "    test_true, test_pred = pred_inverse_all(test_data, model, in_vars_future, out_vars, in_scaler, out_scaler, lag, delay, p_steps, in_vars_past=in_vars_past)\n",
    "    \n",
    "    train_resid = train_true - train_pred\n",
    "    # val_resid = val_true - val_pred\n",
    "    test_resid = test_true - test_pred\n",
    "    train_resid = train_resid.flatten()\n",
    "    # val_resid = val_resid.flatten()\n",
    "    test_resid = test_resid.flatten()\n",
    "\n",
    "    train_eval = mae_mse_rmse(train_true, train_pred)\n",
    "    # val_eval = mae_mse_rmse(val_true, val_pred)\n",
    "    test_eval = mae_mse_rmse(test_true, test_pred)\n",
    "    \n",
    "    m.update({'seq_test':seq_test, 'seq_test_trans':seq_test_trans,\n",
    "                    'seq_train':seq_train, 'seq_train_trans':seq_train_trans,\n",
    "                    # 'seq_val':seq_val, 'seq_val_trans':seq_val_trans,\n",
    "                    'train_true':train_true, 'train_pred':train_pred, \n",
    "                    # 'val_true':val_true, 'val_pred':val_pred, \n",
    "                    'test_true':test_true, 'test_pred':test_pred,\n",
    "                    'train_resid':train_resid, 'test_resid':test_resid,\n",
    "                    # 'val_resid':val_resid, 'val_eval':val_eval, \n",
    "                    'train_eval':train_eval, 'test_eval':test_eval, \n",
    "                    'test_pred_list': test_pred_list,'print_name':model_alias[models.index(m)]})\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da9ad8a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# out_scaler = models[0]['out_scaler']\n",
    "# model = models[0]['model']\n",
    "# in_vars = models[0]['in_vars']\n",
    "# out_vars = models[0]['out_vars']\n",
    "# in_scaler = models[0]['in_scaler']\n",
    "# out_scaler = models[0]['out_scaler']\n",
    "# lag = models[0]['lag']\n",
    "# delay = models[0]['delay']\n",
    "# p_steps = models[0]['prediction_steps']\n",
    "# test_data = models[0]['test_data']\n",
    "# pred_list = pred_all_list(model, out_scaler, seq_test, seq_test_trans)\n",
    "# pred_list2 = pred_and_add_durIndex(model, out_scaler, seq_test, seq_test_trans)\n",
    "# true3,  pred_list3 = pred_inverse_all([test_data[0]], model, in_vars, out_vars, in_scaler, out_scaler, lag, delay, p_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786e05dc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pred_list = models[0]['test_pred_list']\n",
    "# print(pred_list[0][3][10])\n",
    "# print(pred_list2[0][3][10])\n",
    "# print(pred_list3[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418f0f35",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print all Event meta data \n",
    "# i=0\n",
    "# for obj in seq_test_trans:\n",
    "#     print(i, obj[0])\n",
    "#     i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9184f5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Print model summary\n",
    "display(Markdown(\"## **Modellzusammenfassungen** \"))\n",
    "for m in models:\n",
    "    text = \"### **\" + m['name'] + \"**\"\n",
    "    display(Markdown(text))\n",
    "    try:\n",
    "        total_train_time = round(int(m['total_train_time']) // 60, 0)\n",
    "    except:\n",
    "        total_train_time = 'N/A'\n",
    "        pass\n",
    "    head = f\"**Alias: {m['print_name']}**<br><br>Gesamte Trainingszeit: {total_train_time} min\"\n",
    "    display(Markdown(head))\n",
    "    opti = m['model'].optimizer.get_config()\n",
    "    print(\"Optimierungsmethode: \", opti['name'])\n",
    "    print(\"Verlustfunktion: \", m['model'].loss)\n",
    "    utils.plot_model(m['model'], to_file='model.png', show_shapes=True)\n",
    "    display(Markdown('**Netzaufbau:**'))\n",
    "    display(Image('model.png', width=300))\n",
    "    # model_sum = m['model'].summary()\n",
    "    # print(tabulate(model_sum))\n",
    "    print(\"______________________________________________________________________________________________\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf09a3a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train times\n",
    "import random\n",
    "times = pd.DataFrame(columns=['Model', 'Train Time'])\n",
    "for m in models:\n",
    "    try:\n",
    "        total_train_time = round(int(m['total_train_time']) // 60, 0)\n",
    "    except:\n",
    "        total_train_time = 'N/A'\n",
    "        pass\n",
    "    new_row = pd.DataFrame([{'Model': m['print_name'], 'Train Time': total_train_time }])\n",
    "    times = pd.concat([times,new_row], ignore_index=True)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(7, 2))\n",
    "plt.barh(times['Model'], times['Train Time'])\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Zeit - T [min]')\n",
    "plt.ylabel('Modell')\n",
    "plt.title('Gesamte Trainingszeiten')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b269ab7e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Datenaufteilung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42de6c67",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract total precipitation and max intensity from seq_test\n",
    "seq_test_trans = models[0]['seq_test_trans']\n",
    "e2_precipitation = [obj[0]['total precipitation'] for obj in seq_test_trans if obj[0]['event type'] == 'Euler Typ 2']\n",
    "e2_intensity = [obj[0]['max intensity'] for obj in seq_test_trans if obj[0]['event type'] == 'Euler Typ 2']\n",
    "\n",
    "aufgezeichnet_precipitation = [obj[0]['total precipitation'] for obj in seq_test_trans if obj[0]['event type'] == 'Aufgezeichnet']\n",
    "aufgezeichnet_intensity = [obj[0]['max intensity'] for obj in seq_test_trans if obj[0]['event type'] == 'Aufgezeichnet']\n",
    "\n",
    "# Plot events precipitation and intensity in scatterplot\n",
    "fig, ax2 = plt.subplots(figsize=(5, 5))\n",
    "ax2.plot( e2_intensity, e2_precipitation,'o', color='orange', label='Sample')\n",
    "ax2.scatter(aufgezeichnet_intensity, aufgezeichnet_precipitation, marker='o',  label='Events')\n",
    "ax2.set_xlabel('Maximale Niederschlagsintensität iN [mm/h]')\n",
    "ax2.set_ylabel('Niederschlagshöhe hN [mm]')\n",
    "ax2.set_title('Testereignisse', pad=10)\n",
    "ax2.legend(labels=['Euler Typ 2: {}'.format(len(e2_precipitation)), 'Aufgezeichnet: {}'.format(len(aufgezeichnet_precipitation))])\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f4516a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = models[0]\n",
    "train_e2_precip = [obj[0]['total precipitation'] for obj in m['seq_train_trans'] if obj[0]['event type'] == 'Euler Typ 2']\n",
    "train_e2_intensity = [obj[0]['max intensity'] for obj in m['seq_train_trans'] if obj[0]['event type'] == 'Euler Typ 2']\n",
    "\n",
    "train_measured_precip = [obj[0]['total precipitation'] for obj in m['seq_train_trans'] if obj[0]['event type'] == 'Aufgezeichnet']\n",
    "train_measured_intensity = [obj[0]['max intensity'] for obj in m['seq_train_trans'] if obj[0]['event type'] == 'Aufgezeichnet']\n",
    "\n",
    "# val_e2_precip = [obj[0]['total precipitation'] for obj in m['seq_val_trans'] if obj[0]['event type'] == 'Euler Typ 2']\n",
    "# val_e2_intensity = [obj[0]['max intensity'] for obj in m['seq_val_trans'] if obj[0]['event type'] == 'Euler Typ 2']\n",
    "\n",
    "# val_measured_precip = [obj[0]['total precipitation'] for obj in m['seq_val_trans'] if obj[0]['event type'] == 'Aufgezeichnet']\n",
    "# val_measured_intensity = [obj[0]['max intensity'] for obj in m['seq_val_trans'] if obj[0]['event type'] == 'Aufgezeichnet']\n",
    "\n",
    "train_precip = [obj[0]['total precipitation'] for obj in seq_train_trans]\n",
    "train_intensity = [obj[0]['max intensity'] for obj in seq_train_trans]\n",
    "\n",
    "# val_precip = [obj[0]['total precipitation'] for obj in seq_val_trans]\n",
    "# val_intensity = [obj[0]['max intensity'] for obj in seq_val_trans]\n",
    "\n",
    "# Plot events precipitation and intensity in scatterplot\n",
    "fig, ax2 = plt.subplots(figsize=(5, 5))\n",
    "ax2.scatter(train_e2_intensity, train_e2_precip, marker='.', color='orange', label='Sample')\n",
    "ax2.scatter(train_measured_intensity, train_measured_precip, marker='.', label='Events')\n",
    "# ax2.plot( val_e2_intensity, val_e2_precip,'o', color='Black', label='Sample')\n",
    "# ax2.scatter(val_measured_precip, val_measured_intensity, marker='x', color='Black', label='Events')\n",
    "\n",
    "ax2.set_xlabel('Maximale Niederschlagsintensität iN [mm/h]')\n",
    "ax2.set_ylabel('Niederschlagshöhe hN [mm]')\n",
    "ax2.set_title('Trainingsereignisse', pad=10)\n",
    "# ax2.legend(labels=['Training (Euler Typ 2): {}'.format(len(train_e2_intensity)), 'Training (Aufgezeichnet): {}'.format(len(train_measured_precip)), \n",
    "#                 'Validierung (Euler Typ 2): {}'.format(len(val_e2_precip)), 'Validierung (Aufgezeichnet): {}'.format(len(val_measured_intensity))])\n",
    "ax2.legend(labels=['Euler Typ 2: {}'.format(len(train_e2_intensity)), 'Aufgezeichnet: {}'.format(len(train_measured_precip))])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e56be6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    m_name = \"model_\" + str(i)\n",
    "    train_data = model_container[m_name]['train_data']\n",
    "    val_data = model_container[m_name]['validation_data']\n",
    "    seq_train, seq_train_trans = sequence_list(train_data, in_vars_future=in_vars_future, out_vars=out_vars, in_scaler=in_scaler, \n",
    "                                    out_scaler=out_scaler, lag=lag, delay=delay, prediction_steps=p_steps, in_vars_past=in_vars_past)\n",
    "    \n",
    "    # seq_val, seq_val_trans = sequence_list(val_data, in_vars=in_vars, out_vars=out_vars, in_scaler=in_scaler, \n",
    "                                    # out_scaler=out_scaler, lag=lag, delay=delay, prediction_steps=p_steps)\n",
    "    \n",
    "    seq_val, seq_val_trans = sequence_list(val_data, in_vars_future=in_vars_future, out_vars=out_vars, in_scaler=in_scaler, \n",
    "                                    out_scaler=out_scaler, lag=lag, delay=delay, prediction_steps=p_steps, in_vars_past=in_vars_past)\n",
    "\n",
    "    train_precip = [obj[0]['total precipitation'] for obj in seq_train_trans]\n",
    "    train_intensity = [obj[0]['max intensity'] for obj in seq_train_trans]\n",
    "\n",
    "    val_precip = [obj[0]['total precipitation'] for obj in seq_val_trans]\n",
    "    val_intensity = [obj[0]['max intensity'] for obj in seq_val_trans]\n",
    "\n",
    "    # Plot events precipitation and intensity in scatterplot\n",
    "    fig, ax2 = plt.subplots(figsize=(4, 4))\n",
    "    ax2.scatter(train_intensity, train_precip, marker='o', label='Sample')\n",
    "    ax2.scatter(val_intensity, val_precip, marker='x', color='black', label='Events')\n",
    "    # ax2.plot( val_e2_intensity, val_e2_precip,'o', color='Black', label='Sample')\n",
    "    # ax2.scatter(val_measured_precip, val_measured_intensity, marker='x', color='Black', label='Events')\n",
    "\n",
    "    ax2.set_xlabel('Maximale Niederschlagsintensität iN [mm/h]')\n",
    "    ax2.set_ylabel('Niederschlagshöhe hN [mm]')\n",
    "    part_name = \"Kreuzvalidierung - Aufteilung \" + str(i+1)\n",
    "    ax2.set_title(part_name, pad=10)\n",
    "    # ax2.legend(labels=['Training (Euler Typ 2): {}'.format(len(train_e2_intensity)), 'Training (Aufgezeichnet): {}'.format(len(train_measured_precip)), \n",
    "    #                 'Validierung (Euler Typ 2): {}'.format(len(val_e2_precip)), 'Validierung (Aufgezeichnet): {}'.format(len(val_measured_intensity))])\n",
    "    ax2.legend(labels=['Training: {}'.format(len(train_intensity)), 'Validierung: {}'.format(len(val_intensity))])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bb1683",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Gesamtauswertung der Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e05beb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cross validation results\n",
    "display(Markdown('**Auswertung der Metriken bei der Kreuzvalidierung**'))\n",
    "display(Markdown('Bei jeder Modellerstellung werden die Daten zufällig in Trainings- und Validierungsdaten aufgeteilt. Der Boxplot beinhaltet die Metriken der Kreuzvalidierung für die Trainings- und Validierungsdaten.'))\n",
    "display(Markdown('Hierbei ist zu beachten, dass diese Metrik vor der Datentransformation berechnet wurde und somit nur in diesem Verlgleich aussagekraft habe und mit nachfolgenden Metriken nicht vergleichbar ist.'))\n",
    "\n",
    "colors = ['blue','orange', 'green', 'pink', 'lightblue', 'lightgreen', 'violet']\n",
    "\n",
    "if 'mse' in models[0]['cv_scores'].columns:\n",
    "    metrics = ['mse', 'mae']\n",
    "else:\n",
    "    metrics = ['loss']\n",
    "metrics = ['mse', 'mae']\n",
    "metrics_k_fold = {'mae': 'Mittlerer absoluter Fehler - MAE [-]', \n",
    "                  'mse': 'Wurzel der mittleren Fehlerquadratsumme - RMSE [-]',}\n",
    "\n",
    "\n",
    "# metric_name = 'mse'\n",
    "for metric_name in metrics:\n",
    "    if metric_name == 'mse':\n",
    "        metric_upper = 'RMSE'\n",
    "    else:\n",
    "        metric_upper = metric_name.upper()\n",
    "    data_groups = []\n",
    "    model_names = []\n",
    "    for m in models:\n",
    "        model_names.append(m['print_name'])\n",
    "        train_result = m['cv_scores'][metric_name]\n",
    "        val_result = m['cv_scores']['val_'+ metric_name]\n",
    "        if metric_name == 'mse':\n",
    "            train_result = np.sqrt(train_result)\n",
    "            val_result = np.sqrt(val_result)\n",
    "\n",
    "\n",
    "        new_data = [train_result, val_result]\n",
    "        data_groups.append(new_data)\n",
    "\n",
    "\n",
    "    # --- Labels for your data:\n",
    "    labels_list = ['Training','Validierung']\n",
    "    width       = 1/len(labels_list)\n",
    "    xlocations  = [ x*((1+ len(data_groups))*width) for x in range(len(new_data)) ]\n",
    "\n",
    "    symbol      = 'r+'\n",
    "    ymin        = min ( [ val  for dg in data_groups  for data in dg for val in data ] )\n",
    "    ymax        = max ( [ val  for dg in data_groups  for data in dg for val in data ])\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.set_ylim(ymin,ymax)\n",
    "\n",
    "    ax.grid(True, linestyle='dotted')\n",
    "    ax.set_axisbelow(True)\n",
    "\n",
    "    # plt.xlabel('X axis label')\n",
    "    plt.ylabel(metrics_k_fold[metric_name], fontsize=11)\n",
    "    plt.title('title')\n",
    "\n",
    "    space = len(data_groups)/2\n",
    "    offset = len(data_groups)/2\n",
    "\n",
    "\n",
    "    # --- Offset the positions per group:\n",
    "\n",
    "    group_positions = []\n",
    "    for num, dg in enumerate(data_groups):    \n",
    "        _off = (0 - space + (0.5+num))\n",
    "        group_positions.append([x+_off*(width+0.01) for x in xlocations])\n",
    "\n",
    "    for dg, pos, c, name in zip(data_groups, group_positions, colors, model_names):\n",
    "        boxes = ax.boxplot(dg, \n",
    "                    sym=symbol,\n",
    "                    labels=['']*len(labels_list),\n",
    "        #            labels=labels_list,\n",
    "                    positions=pos, \n",
    "                    widths=width, \n",
    "                    boxprops=dict(facecolor=c),\n",
    "        #             capprops=dict(color=c),\n",
    "        #            whiskerprops=dict(color=c),\n",
    "        #            flierprops=dict(color=c, markeredgecolor=c),                       \n",
    "                    medianprops=dict(color='black'),\n",
    "        #           notch=False,  \n",
    "        #           vert=True, \n",
    "        #           whis=1.5,\n",
    "        #           bootstrap=None, \n",
    "        #           usermedians=None, \n",
    "        #           conf_intervals=None,\n",
    "                    patch_artist=True,\n",
    "                    )\n",
    "        plt.plot([], c=c, label=name)\n",
    "    plt.legend(bbox_to_anchor=(0.5, -0.1), loc='upper center', ncol=3, title='Modell', fontsize=11, title_fontsize=11)\n",
    "    ax.set_xticks( xlocations )\n",
    "    ax.set_xticklabels( labels_list, rotation=0 , fontsize=11)\n",
    "    ax.set_title(metric_upper + ' bei 5-Facher Kreuzvalidierung', pad=10, fontsize=13)\n",
    "    ymax = max([val for dg in data_groups for data in dg for val in data])\n",
    "    ymax = ymax + ymax * 0.05\n",
    "    ax.set_ylim(0, ymax)\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c79abd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # cross validation results\n",
    "# display(Markdown('**Auswertung des durchschnittlichen Metriken der Kreuzvalidierung**'))\n",
    "# display(Markdown('Bei jeder Modellerstellung werden die Daten zufällig in Trainings- und Validierungsdaten aufgeteilt. Die Balken zeigen die durchschnittlichen Metriken der Kreuzvalidierung für die Trainings- und Validierungsdaten.'))\n",
    "# display(Markdown('Hierbei ist zu beachten, dass diese Metrik vor der Datentransformation berechnet wurde und somit nur in diesem Verlgleich aussagekraft habe und mit nachfolgenden Metriken nicht vergleichbar ist.'))\n",
    "\n",
    "# if 'mse' in models[0]['cv_scores'].columns:\n",
    "#     metrics = ['mse', 'mae']\n",
    "# else:\n",
    "#     metrics = ['loss']\n",
    "# metrics = ['mse', 'mae']\n",
    "# metrics_k_fold = {'mae': 'Mittlerer absoluter Fehler - MAE [-]', \n",
    "#                   'mse': 'Wurzel der mittleren Fehlerquadratsumme - RMSE [-]',}\n",
    "\n",
    "# for metric_name in metrics:\n",
    "#     metric = pd.DataFrame()\n",
    "#     metric_table = pd.DataFrame()\n",
    "#     if metric_name == 'mse':\n",
    "#         metric_upper = 'RMSE'\n",
    "#     else:\n",
    "#         metric_upper = metric_name.upper()\n",
    "#     for m in models:\n",
    "#         name = m['print_name']\n",
    "#         train_result = m['cv_scores'][metric_name].mean()\n",
    "#         if metric_name == 'mse':\n",
    "#             train_result = np.sqrt(train_result)\n",
    "#         val_result = m['cv_scores']['val_' + metric_name].mean()\n",
    "#         if metric_name == 'mse':\n",
    "#             val_result = np.sqrt(val_result)\n",
    "#         new_row = pd.DataFrame({'Modell':name,'Training': train_result, 'Validierung': val_result}, index=[0])\n",
    "#         if val_result < 1 or train_result < 1:\n",
    "#             metric = pd.concat([metric, pd.DataFrame(new_row)] , ignore_index=True)\n",
    "#             metric_table = pd.concat([metric_table, pd.DataFrame(new_row)] , ignore_index=True)\n",
    "#         else:\n",
    "#             metric_table = pd.concat([metric_table, pd.DataFrame(new_row)] , ignore_index=True)\n",
    "\n",
    "#     ax = metric.set_index('Modell').T.plot.bar(alpha=.7, rot=0, stacked=False, figsize=(5,4))\n",
    "#     ax.legend(bbox_to_anchor=(0.5, -0.1), loc='upper center', ncol=3, title='Modell')\n",
    "#     ax.set_ylabel(metrics_k_fold[metric_name])\n",
    "\n",
    "#     ax.set_title('Durchschnittl. '+ metric_upper + ' der Kreuzvalidierung', pad=10)\n",
    "#     display(Markdown('Durchschnittlicher ' + metric_upper + ':'))\n",
    "#     print(tabulate(round(metric_table, 5), headers='keys', tablefmt='pretty', showindex=False))\n",
    "#     # Show the plot\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3313a13",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the learning curve\n",
    "display(Markdown('### Lernkurven'))\n",
    "metrics = ['mse', 'mae']\n",
    "for m in models:\n",
    "    metric = 'loss'\n",
    "    train_switch = len(m['combined_history'][metric]) - len(m['history'][metric])\n",
    "    pyplot.plot(np.sqrt(m['combined_history']['loss']), '--', label='Training')\n",
    "    if 'val_'+metric in m['combined_history']:\n",
    "        pyplot.plot(np.sqrt(m['combined_history']['val_'+metric]), label='Test')\n",
    "    pyplot.axvline(x=train_switch, color='grey', linestyle='--', linewidth=1, label='Ende Vorauswahl/\\nStart Haupttraining')\n",
    "    pyplot.xlabel('Trainingsepoche')\n",
    "    pyplot.ylabel('Mittlerer quadratischer Fehler - RMSE [-]')\n",
    "    pyplot.legend()\n",
    "    pyplot.title('Lernkurve - Modell: ' + m['print_name'])\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e497fd4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Vergleich RMSE\n",
    "metric_names = ('rmse', 'mae')\n",
    "for m_name in metric_names:\n",
    "    metrics = pd.DataFrame()\n",
    "    name_upper = m_name.upper()\n",
    "    for m in models:\n",
    "        name = m['print_name']\n",
    "        train_metric = m['train_eval'][m_name]\n",
    "        # val_metric = m['val_eval'][m_name]\n",
    "        test_metric = m['test_eval'][m_name]\n",
    "        new_row = pd.DataFrame({'Modell':name,'Training': train_metric, 'Test': test_metric}, index=[0])\n",
    "        # new_row = pd.DataFrame({'Modell':name,'Training': train_metric, 'Validierung': val_metric, 'Test': test_metric}, index=[0])\n",
    "        metrics = pd.concat([metrics, pd.DataFrame(new_row)] , ignore_index=True)\n",
    "\n",
    "    ax = metrics.set_index('Modell').T.plot.bar(alpha=.7, rot=0, stacked=False, figsize=(5,4.5))\n",
    "    ax.legend(bbox_to_anchor=(0.5, -0.1), loc='upper center', ncol=3, title='Modell')\n",
    "    ax.set_ylabel(metrics_labels[m_name])\n",
    "\n",
    "    ax.set_title(name_upper + ' des besten Modells ', pad=15)\n",
    "\n",
    "    # print(name_upper)\n",
    "    display(Markdown('**Auswertung nach '+name_upper+'**'))\n",
    "    print(tabulate(round(metrics, 4), headers='keys', tablefmt='pretty', showindex=False))\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425bbbc1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Modellauswertung nach Größenklassen der Ausgabewerte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c83595",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Metrics based on Output magnitude\n",
    "magnitude_metrics = []\n",
    "\n",
    "for m in models:\n",
    "\n",
    "    test_true_flattened = m['test_true'].flatten()\n",
    "    test_pred_flattened = m['test_pred'].flatten()\n",
    "\n",
    "    test_stack_flat = np.column_stack((test_true_flattened, test_pred_flattened))\n",
    "    test_stack_flat = test_stack_flat[test_stack_flat[:, 0] != 0]\n",
    "    test_stack_flat = pd.DataFrame(test_stack_flat, columns=['True', 'Pred'])\n",
    "\n",
    "    # Get the value range of test_stack['True']\n",
    "    value_range = np.ptp(test_stack_flat['True'])\n",
    "\n",
    "    # Split the value range into 4 equal ranges\n",
    "    ranges = np.linspace(0, np.max(test_stack_flat['True']), num=11)\n",
    "\n",
    "\n",
    "    test_stack_flat['Bins'] = pd.cut(test_stack_flat['True'], bins=ranges, labels=False, include_lowest=True)\n",
    "    ranges = np.round(ranges, 2)\n",
    "\n",
    "    # Get unique bins\n",
    "    unique_bins = test_stack_flat['Bins'].unique()\n",
    "    unique_bins = sorted(unique_bins)\n",
    "\n",
    "\n",
    "\n",
    "    # Create a list to store the datasets\n",
    "    bins = []\n",
    "\n",
    "    # Iterate over each unique bin\n",
    "    for bin_num in unique_bins:\n",
    "        # Filter the dataframe for the current bin\n",
    "        bin_data = test_stack_flat[test_stack_flat['Bins'] == bin_num].copy()\n",
    "        \n",
    "        # Append the filtered dataset to the list\n",
    "        bins.append(bin_data)\n",
    "\n",
    "    metrics = pd.DataFrame([])\n",
    "    metrics = pd.DataFrame(columns=['Bin', 'MSE','RMSE', 'MAE', 'MAPE', 'start', 'end', 'False_0', 'n'])\n",
    "    i = 0\n",
    "    for bin in bins:\n",
    "        mse = np.mean((bin['True'] - bin['Pred'])**2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = np.mean(np.abs(bin['True'] - bin['Pred']))\n",
    "        mape = np.mean(np.abs((bin['True'] - bin['Pred']) / bin['True'])) * 100\n",
    "        \n",
    "        bin_start = ranges[i]\n",
    "        bin_end = ranges[i+1]\n",
    "        False_0 = len(bin[bin['Pred'] == 0])\n",
    "        n = len(bin)\n",
    "\n",
    "        new_row = pd.DataFrame({'Bin':i, 'MSE':mse,'RMSE':rmse, 'MAE':mae, 'MAPE':mape, 'start':bin_start, 'end':bin_end, 'False_0': False_0, 'n':n}, index=[0])\n",
    "\n",
    "        i +=1\n",
    "        \n",
    "        metrics = pd.concat([metrics,new_row], ignore_index=True)\n",
    "        \n",
    "    \n",
    "    magnitude_metrics.append(metrics)\n",
    "\n",
    "\n",
    "\n",
    "i = 0\n",
    "for metric in magnitude_metrics:\n",
    "    metric[['MSE', 'MAE', 'RMSE']] = metric[['MSE', 'MAE', 'RMSE']].round(4)\n",
    "    metric[['MAPE']] = metric[['MAPE']].round(2)\n",
    "    print('Modell: ' + models[i]['print_name'])\n",
    "    print(tabulate(metric, headers='keys', tablefmt='pretty', showindex=False))\n",
    "    i += 1\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f6f9dd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics = ('MAPE', 'MAE', 'RMSE')\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(10, 5))\n",
    "\n",
    "j = 0\n",
    "for metric in metrics:\n",
    "    i = 0\n",
    "    for m in magnitude_metrics:\n",
    "        if metric == 'MAPE':\n",
    "            x = m['end'].iloc[1:]\n",
    "            y = m[metric].iloc[1:]\n",
    "        else:\n",
    "            x = m['end']\n",
    "            y = m[metric]\n",
    "        \n",
    "        # print(m)\n",
    "        axs[j].scatter(x, y, label = models[i]['print_name'], marker='o', s=40)\n",
    "        axs[j].plot(x, y, linewidth=0.5)\n",
    "        i += 1\n",
    "    axs[j].set_xlabel('Q - Wahr ['+ out_unit + ']')\n",
    "    axs[j].set_ylabel(metrics_labels[metric.lower()])\n",
    "    for border in magnitude_metrics[0]['end']:\n",
    "        axs[j].axvline(x=border, color = 'grey', linestyle='--', linewidth=0.5)\n",
    "    axs[j].set_title(metric + ' vs. Wahr')\n",
    "    axs[j].set_xlim(0,None)\n",
    "\n",
    "    axs[j].legend(title=\"Modell\", fancybox=True)._legend_box.align = \"left\"\n",
    "    \n",
    "    j += 1\n",
    "\n",
    "fig.suptitle('Auswertung nach Größenklassen der Ausgabewerte', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450531bd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Residuen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7501535c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Plot True vs. Predicted\n",
    "# cols = ['Modell: {}'.format(m_name['print_name']) for m_name in models]\n",
    "# rows = ['{}'.format(row) for row in ['Training', 'Test']]\n",
    "# # rows = ['{}'.format(row) for row in ['Training', 'Validierung', 'Test']]\n",
    "\n",
    "# m_trues = []\n",
    "# m_preds = []\n",
    "# mins =[]\n",
    "# maxs = []\n",
    "\n",
    "# for m in models:\n",
    "#     # Calculate the regression line using polynomial fit\n",
    "#     train_pred_flat = m['train_pred'].flatten()\n",
    "#     train_true_flat = m['train_true'].flatten()\n",
    "#     # val_pred_flat = m['val_pred'].flatten()\n",
    "#     # val_true_flat = m['val_true'].flatten()\n",
    "#     test_pred_flat = m['test_pred'].flatten()\n",
    "#     test_true_flat = m['test_true'].flatten()\n",
    "#     names = ['Training', 'Test']\n",
    "#     preds = [train_pred_flat, test_pred_flat]\n",
    "#     trues = [train_true_flat, test_true_flat]\n",
    "#     # names = ['Training', 'Validierung', 'Test']\n",
    "#     # preds = [train_pred_flat, val_pred_flat, test_pred_flat]\n",
    "#     # trues = [train_true_flat, val_true_flat, test_true_flat]\n",
    "#     min_value = min(min(r) for r in preds)\n",
    "#     max_value = max(max(r) for r in preds)\n",
    "#     mins.append(min_value)\n",
    "#     maxs.append(max_value)\n",
    "#     m_preds.append(preds)\n",
    "#     m_trues.append(trues)\n",
    "\n",
    "# min_val = min(mins)\n",
    "# max_val = max(maxs)\n",
    "# regression_range = np.linspace(min_val, max_val, 100)\n",
    "\n",
    "# # Plot if only one model is given\n",
    "# if len(m_preds) == 1:\n",
    "#     for j in range(len(m_preds)):\n",
    "#         fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "\n",
    "#         for i in range(len(m_preds[j])):\n",
    "#             # Create a figure and subplots\n",
    "\n",
    "#             reg_line = np.polyfit(m_preds[j][i], m_trues[j][i], 2)\n",
    "#             reg_line_fn = np.poly1d(reg_line)\n",
    "#             # Plot train_pred/train_resid\n",
    "#             axs[i].scatter(m_preds[j][i], m_trues[j][i], marker ='.', s= 2, label = 'Wertepaare')\n",
    "#             axs[i].set_title(names[i])\n",
    "#             axs[i].set_xlabel('Q - vorhergesagt ['+ out_unit + ']')\n",
    "#             axs[i].set_ylabel('Q - Wahr ['+ out_unit + ']')    \n",
    "#             # Plot the average line\n",
    "#             axs[i].plot([0, max_val], [0, max_val], color='black', linestyle='--', label='Optimal')\n",
    "#             # Plot the regression line\n",
    "#             axs[i].plot(regression_range, reg_line_fn(regression_range), color='red', label='Regression mit\\nPolynom 2. Grades')\n",
    "#             axs[i].legend(frameon=False)\n",
    "#             axs[i].set_ylim(min_val, max_val)\n",
    "#             axs[i].legend()\n",
    "\n",
    "#     fig.suptitle('Wahr vs. Vorhergesagt - ' + models[j]['print_name'], fontsize=16)\n",
    "#     plt.show()\n",
    "\n",
    "# # Plot if more than one model are given\n",
    "# else:\n",
    "#     for j in range(len(m_preds[0])):\n",
    "#         fig, axs = plt.subplots(ncols=len(models),figsize=(10, 5))\n",
    "\n",
    "#         for i in range(len(m_preds)):\n",
    "#             try:\n",
    "#                 # Create a figure and subplots\n",
    "\n",
    "#                 reg_line = np.polyfit(m_preds[i][j], m_trues[i][j], 2)\n",
    "#                 reg_line_fn = np.poly1d(reg_line)\n",
    "#                 # Plot train_pred/train_resid\n",
    "#                 axs[i].scatter(m_preds[i][j], m_trues[i][j], marker ='.', s= 2, label = 'Wertepaare')\n",
    "#                 axs[i].set_title(cols[i])\n",
    "#                 axs[i].set_xlabel('Q - vorhergesagt ['+ out_unit + ']')\n",
    "#                 axs[i].set_ylabel('Q - Wahr ['+ out_unit + ']')    \n",
    "#                 # Plot the average line\n",
    "#                 axs[i].plot([0, max_val], [0, max_val], color='black', linestyle='--', label='Optimal')\n",
    "#                 # Plot the regression line\n",
    "#                 axs[i].plot(regression_range, reg_line_fn(regression_range), color='red', label='Regression mit\\nPolynom 2. Grades')\n",
    "#                 axs[i].legend(frameon=False)\n",
    "#                 axs[i].set_ylim(min_val, max_val)\n",
    "#                 axs[i].legend()\n",
    "#             except:\n",
    "#                 pass\n",
    "#         fig.suptitle('Wahr vs. Vorhergesagt - ' + rows[j], fontsize=16)\n",
    "#         fig.tight_layout()\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f5656c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols = ['Modell: {}'.format(m_name['print_name']) for m_name in models]\n",
    "rows = ['{}'.format(row) for row in ['Test']]\n",
    "# rows = ['{}'.format(row) for row in ['Training', 'Validierung', 'Test']]\n",
    "\n",
    "m_resids = []\n",
    "m_preds = []\n",
    "mins =[]\n",
    "maxs = []\n",
    "maxs_preds = []\n",
    "for m in models:\n",
    "    # Calculate the regression line using polynomial fit\n",
    "    # train_pred_flat = m['train_pred'].flatten()\n",
    "    # train_resid = m['train_resid']\n",
    "    # val_pred_flat = m['val_pred'].flatten()\n",
    "    # val_resid = m['val_resid']\n",
    "    test_pred_flat = m['test_pred'].flatten()\n",
    "    test_resid = m['test_resid']\n",
    "    # names = ['Training', 'Test']\n",
    "    # preds = [train_pred_flat, test_pred_flat]\n",
    "    # resids = [train_resid, test_resid]\n",
    "    names = ['Datensatz: Test']\n",
    "    preds = [test_pred_flat]\n",
    "    resids = [test_resid]\n",
    "    # names = ['Training', 'Validierung', 'Test']\n",
    "    # preds = [train_pred_flat, val_pred_flat, test_pred_flat]\n",
    "    # resids = [train_resid, val_resid, test_resid]\n",
    "    min_value = min(min(r) for r in resids)\n",
    "    max_value = max(max(r) for r in resids)\n",
    "    max_pred = max(max(p) for p in preds)\n",
    "    mins.append(min_value)\n",
    "    maxs.append(max_value)\n",
    "    maxs_preds.append(max_pred)\n",
    "    m_preds.append(preds)\n",
    "    m_resids.append(resids)\n",
    "max_pred_val = max(maxs_preds)\n",
    "min_val = min(mins)\n",
    "max_val = max(maxs)\n",
    "regression_range = np.linspace(0, max_pred_val, 100)\n",
    "\n",
    "\n",
    "\n",
    "# Plot if only one model is given\n",
    "if len(m_preds) == 1:\n",
    "    for j in range(len(m_preds)):\n",
    "        fig, axs = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "        for i in range(len(m_preds[j])):\n",
    "            try:\n",
    "                # Create a figure and subplots\n",
    "                \n",
    "                reg_line = np.polyfit(m_preds[j][i], m_resids[j][i], 2)\n",
    "                reg_line_fn = np.poly1d(reg_line)\n",
    "                # Plot train_pred/train_resid\n",
    "                axs.scatter(m_preds[j][i], m_resids[j][i], marker ='.', s= 2,label='Wertepaare')\n",
    "                axs.set_title('Modell:' + models[j]['print_name'])\n",
    "                axs.set_xlabel('Q - vorhergesagt ['+ out_unit + ']')\n",
    "                if i == 0:\n",
    "                    axs.set_ylabel('Residuen ['+ out_unit + ']')\n",
    "                # Plot the average line\n",
    "                axs.axhline(0, color='black', linestyle='--', label='Optimal')\n",
    "                # Plot the regression line\n",
    "                axs.plot(regression_range, reg_line_fn(regression_range), color='red', label='Regression mit\\nPolynom 2. Grades')\n",
    "                axs.legend(frameon=False)\n",
    "                axs.set_ylim(min_val, max_val)\n",
    "                axs.legend()\n",
    "            except:\n",
    "                pass\n",
    "        fig.suptitle('Residuen vs. Vorhergesagt', fontsize=16)\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "\n",
    "else:\n",
    "# print(len(m_preds[0]))\n",
    "    for j in range(len(m_preds[0])):\n",
    "        # j = 0\n",
    "        fig, axs = plt.subplots(ncols=len(models),figsize=(10, 5))\n",
    "        for i in range(len(m_preds)):\n",
    "        # try:\n",
    "            reg_line = np.polyfit(m_preds[i][j], m_resids[i][j], 2)\n",
    "            reg_line_fn = np.poly1d(reg_line)\n",
    "            # Plot train_pred/train_resid\n",
    "            axs[i].scatter(m_preds[i][j], m_resids[i][j], marker ='.', s= 2,label='Wertepaare')\n",
    "            axs[i].set_title('Modell: ' + models[i]['print_name'])\n",
    "            axs[i].set_xlabel('Q - vorhergesagt ['+ out_unit + ']')\n",
    "            if i == 0:\n",
    "                axs[i].set_ylabel('Residuen ['+ out_unit + ']')\n",
    "            # Plot the average line\n",
    "            axs[i].axhline(0, color='black', linestyle='--', label='Optimal')\n",
    "            # Plot the regression line\n",
    "            axs[i].plot(regression_range, reg_line_fn(regression_range), color='red', label='Regression mit\\nPolynom 2. Grades')\n",
    "            axs[i].legend(frameon=False)\n",
    "            axs[i].set_ylim(min_val, max_val)\n",
    "            axs[i].legend()\n",
    "        # except:\n",
    "        #     pass\n",
    "        fig.suptitle('Residuen vs. Vorhergesagt', fontsize=16)\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# print(max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecaf19f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot True vs. Predicted\n",
    "\n",
    "# cols = ['Modell: {}'.format(m_name['print_name']) for m_name in models]\n",
    "# rows = ['{}'.format(row) for row in ['Training', 'Test']]\n",
    "# # rows = ['{}'.format(row) for row in ['Training', 'Validierung', 'Test']]\n",
    "\n",
    "# m_resids = []\n",
    "# m_preds = []\n",
    "# mins =[]\n",
    "# maxs = []\n",
    "# maxs_preds = []\n",
    "# for m in models:\n",
    "#     # Calculate the regression line using polynomial fit\n",
    "#     train_pred_flat = m['train_pred'].flatten()\n",
    "#     train_resid = m['train_resid']\n",
    "#     # val_pred_flat = m['val_pred'].flatten()\n",
    "#     # val_resid = m['val_resid']\n",
    "#     test_pred_flat = m['test_pred'].flatten()\n",
    "#     test_resid = m['test_resid']\n",
    "#     names = ['Training', 'Test']\n",
    "#     preds = [train_pred_flat, test_pred_flat]\n",
    "#     resids = [train_resid, test_resid]\n",
    "#     # names = ['Training', 'Validierung', 'Test']\n",
    "#     # preds = [train_pred_flat, val_pred_flat, test_pred_flat]\n",
    "#     # resids = [train_resid, val_resid, test_resid]\n",
    "#     min_value = min(min(r) for r in resids)\n",
    "#     max_value = max(max(r) for r in resids)\n",
    "#     max_pred = max(max(p) for p in preds)\n",
    "#     mins.append(min_value)\n",
    "#     maxs.append(max_value)\n",
    "#     maxs_preds.append(max_pred)\n",
    "#     m_preds.append(preds)\n",
    "#     m_resids.append(resids)\n",
    "# max_pred_val = max(maxs_preds)\n",
    "# min_val = min(mins)\n",
    "# max_val = max(maxs)\n",
    "# regression_range = np.linspace(0, max_pred_val, 100)\n",
    "\n",
    "# # Plot if only one model is given\n",
    "# if len(m_preds) == 1:\n",
    "#     for j in range(len(m_preds)):\n",
    "#         fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "\n",
    "#         for i in range(len(m_preds[j])):\n",
    "#             try:\n",
    "#                 # Create a figure and subplots\n",
    "                \n",
    "#                 reg_line = np.polyfit(m_preds[j][i], m_resids[j][i], 2)\n",
    "#                 reg_line_fn = np.poly1d(reg_line)\n",
    "#                 # Plot train_pred/train_resid\n",
    "#                 axs[i].scatter(m_preds[j][i], m_resids[j][i], marker ='.', s= 2,label='Wertepaare')\n",
    "#                 axs[i].set_title(names[i])\n",
    "#                 axs[i].set_xlabel('Q - vorhergesagt ['+ out_unit + ']')\n",
    "#                 if i == 0:\n",
    "#                     axs[i].set_ylabel('Residuen ['+ out_unit + ']')\n",
    "#                 # Plot the average line\n",
    "#                 axs[i].axhline(np.mean(m_resids[j][i]), color='black', linestyle='--', label='Optimal')\n",
    "#                 # Plot the regression line\n",
    "#                 axs[i].plot(regression_range, reg_line_fn(regression_range), color='red', label='Regression mit\\nPolynom 2. Grades')\n",
    "#                 axs[i].legend(frameon=False)\n",
    "#                 axs[i].set_ylim(min_val, max_val)\n",
    "#                 axs[i].legend()\n",
    "#             except:\n",
    "#                 pass\n",
    "#         fig.suptitle('Residuen vs. Vorhergesagt - ' + models[j]['print_name'], fontsize=16)\n",
    "#         # Show the plot\n",
    "#         plt.show()\n",
    "\n",
    "# #a.x2 + b.x + c\n",
    "# # Plot if more than one model are given\n",
    "# else:\n",
    "#     for j in range(len(m_preds[0])):\n",
    "#         fig, axs = plt.subplots(ncols=len(models),figsize=(10, 5))\n",
    "#         for i in range(len(m_preds)):\n",
    "#             try:\n",
    "#                 reg_line = np.polyfit(m_preds[i][j], m_resids[i][j], 2)\n",
    "#                 reg_line_fn = np.poly1d(reg_line)\n",
    "#                 # Plot train_pred/train_resid\n",
    "#                 axs[i].scatter(m_preds[i][j], m_resids[i][j], marker ='.', s= 2,label='Wertepaare')\n",
    "#                 axs[i].set_title(cols[i])\n",
    "#                 axs[i].set_xlabel('Q - vorhergesagt ['+ out_unit + ']')\n",
    "#                 if i == 0:\n",
    "#                     axs[i].set_ylabel('Residuen ['+ out_unit + ']')\n",
    "#                 # Plot the average line\n",
    "#                 axs[i].axhline(np.mean(m_resids[i][j]), color='black', linestyle='--', label='Optimal')\n",
    "#                 # Plot the regression line\n",
    "#                 axs[i].plot(regression_range, reg_line_fn(regression_range), color='red', label='Regression mit\\nPolynom 2. Grades')\n",
    "#                 axs[i].legend(frameon=False)\n",
    "#                 axs[i].set_ylim(min_val, max_val)\n",
    "#                 axs[i].legend()\n",
    "#             except:\n",
    "#                 pass\n",
    "#         fig.suptitle('Residuen vs. Vorhergesagt - ' + rows[j], fontsize=16)\n",
    "#         fig.tight_layout()\n",
    "#         plt.show()\n",
    "\n",
    "# # print(max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42718ba2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# n_sample = 3\n",
    "# n = 15\n",
    "# # test for plot_seq_i_d_Q function\n",
    "# for m in models:\n",
    "#     seq_test = m['seq_test']\n",
    "#     seq_test_trans = m['seq_test_trans']\n",
    "#     model = m['model']\n",
    "\n",
    "#     interval = seq_test[n_sample][0]['interval']\n",
    "#     meta = seq_test[n_sample][0]\n",
    "#     in_seq = seq_test[n_sample][1][n]\n",
    "#     out_act = seq_test[n_sample][2][n]\n",
    "\n",
    "#     # Calculate the start and end time of the sequence so that a duration column can be created\n",
    "#     start_time = max(seq_test[n_sample][1][n][:, 0]) + interval + delay * interval\n",
    "#     end_time = start_time + p_steps * interval\n",
    "#     duration_col = np.arange(start_time, end_time, interval)\n",
    "\n",
    "#     Predict = model.predict(seq_test_trans[n_sample][1])\n",
    "#     Predict_invert = out_scaler.inverse_transform(Predict)\n",
    "\n",
    "#     predict_seq = np.column_stack((duration_col,Predict_invert[n]))\n",
    "#     actual_seq = np.column_stack((duration_col,out_act))\n",
    "\n",
    "\n",
    "#     plot_seq_i_d_Q(in_seq, actual_seq, predict_seq, meta, interval,).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b8259f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Nash Sutcliffe Effizienz - NSE\n",
    "Berechnung der mittleren NSE für die Extremwert sequenzen bei denen der Extremwert im 30-minütigen Vorhersagehorizont liegt. <br>\n",
    "Je Ereignis wird somit einmal die NSE berechnet. Bezogen auf die Modellvarianten werden diese dann gemittelt.<br>\n",
    "Bei der Berechnung wird der NSE von Nan-werten und unendlichen Werten bereiningt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcae838e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate nse for all models for test data\n",
    "# there calculate nse for each sequence and take avrage\n",
    "NSE_mean = []\n",
    "for m in models:\n",
    "        interval = m['seq_test'][0][0]['interval']\n",
    "        pred_list = m['test_pred_list']\n",
    "        # set peak horizon = time difference between the peak and the start of the prediction\n",
    "        peak_horizon = 30 # in minutes\n",
    "        peak_horizon_steps = int(peak_horizon / interval)\n",
    "\n",
    "        NSEs_events = []\n",
    "        for event_id in range(len(seq_test_trans)):\n",
    "                \n",
    "                NSEs = np.array([])\n",
    "                # for m in models:\n",
    "                actual = pred_list[event_id][2]\n",
    "                pred    = pred_list[event_id][3]\n",
    "                n = max(range(len(actual)), key=lambda index: np.max(actual[index][:,1]))\n",
    "                n = n - peak_horizon_steps\n",
    "                try:\n",
    "                        nse = 1 - np.sum((actual[n][:,1] - pred[n][:,1])**2) / (np.sum((actual[n][:,1] - np.mean(actual[n][:,1]))**2))\n",
    "                        NSEs = np.append(NSEs, nse)\n",
    "                except:\n",
    "                        pass\n",
    "                # for step in range(p_steps):\n",
    "                #         try:\n",
    "                #                 n = n + step\n",
    "                #                 act_seq = actual[n][:,1]\n",
    "                #                 pred_seq = pred[n][:,1]\n",
    "                #                 # zero_indices = np.where(act_seq != 0)[0]\n",
    "                #                 # act_seq = act_seq[zero_indices]\n",
    "                #                 # pred_seq = pred_seq[zero_indices]\n",
    "                #                 nse = 1 - np.sum((act_seq - pred_seq)**2) / (np.sum((act_seq - np.mean(act_seq))**2))\n",
    "                #                 if np.isnan(nse) == False and np.isinf(nse) == False:\n",
    "                #                         NSEs = np.append(NSEs, nse)\n",
    "\n",
    "                #         except:\n",
    "                #             pass\n",
    "\n",
    "                NSEs_events.append(NSEs)\n",
    "\n",
    "        import math\n",
    "        NSEs = []\n",
    "        for event in NSEs_events:\n",
    "                event = event[np.isfinite(event)]\n",
    "                event = event[~np.isnan(event)]\n",
    "                if math.isnan(np.mean(event)) == False:\n",
    "                        NSEs.append(np.mean(event))\n",
    "                # NSEs.append(np.mean(event))\n",
    "                # print(event)\n",
    "\n",
    "        NSE_mean.append(round(np.mean(NSEs),2))\n",
    "\n",
    "NSEs_df = pd.DataFrame({\n",
    "    'Modell': [models[i]['print_name'] for i in range(len(models))],\n",
    "    'NSE [-]': NSE_mean\n",
    "})\n",
    "print(tabulate(NSEs_df, headers='keys', tablefmt='psql', showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bb8337",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Auswertung der Maximalwertabweichungen\n",
    "Berechnet werden für die Maximalwertabweichungen die Metriken RMSE, MAE sowie MAPE. <br>\n",
    "Zugehörig dazu werden die mittleren Zeitabweichungen (dt) der Maximalwerte ermittelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240e6dd9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot predict and plot the peak time steps of each event and calculate the peak metrics\n",
    "#retrieve the interval of the time steps\n",
    "interval = models[0]['seq_test'][0][0]['interval']\n",
    "p_steps = models[0]['prediction_steps']\n",
    "# set peak horizon = time difference between the peak and the start of the prediction\n",
    "peak_horizon = 30 # in minutes\n",
    "\n",
    "\n",
    "peak_horizon_steps = p_steps - int((peak_horizon) / interval) - 1\n",
    "# Create a grid of subplots\n",
    "\n",
    "abs_errors = [np.array([]) for _ in range(len(models))]\n",
    "sqrt_errors = abs_errors.copy()\n",
    "abs_prct_errors = abs_errors.copy()\n",
    "\n",
    "abs_errors_T = abs_errors.copy()\n",
    "sqrt_errors_T = abs_errors.copy()\n",
    "abs_prct_errors_T = abs_errors.copy()\n",
    "abs_dT = abs_errors.copy()\n",
    "\n",
    "def add_dur(seq , event_id, n):\n",
    "    start_time = max(seq_test[event_id][1][n][:, 0]) + interval + delay * interval\n",
    "    end_time = start_time + p_steps * interval\n",
    "    # Calculate the start and end time of the sequence so that a duration column can be created\n",
    "    duration_col = np.arange(start_time, end_time, interval)\n",
    "\n",
    "    actual_seq = np.column_stack((duration_col,seq))\n",
    "    return actual_seq, duration_col, start_time, end_time\n",
    "\n",
    "event_dicts = []\n",
    "# Loop through n_sample\n",
    "for event_id in range(len(seq_test_trans)):\n",
    "    \n",
    "    dict = {}\n",
    "    # n = sequence index for the sequence containing the peak\n",
    "    max_q = np.max(seq_test[event_id][2][:, :, 0])\n",
    "    \n",
    "    \n",
    "    indices = np.where(seq_test[event_id][2][:, :, 0] == max_q)\n",
    "    # indices[0] will give you the index in the first dimension\n",
    "    n = indices[0][0] if len(indices[0]) > 0 else None\n",
    "    first_max = n\n",
    "    # print(\"first_max: \",n)\n",
    "    len_max = len(indices[0])\n",
    "    # print(\"event: \",event_id, \" max_q: \",max_q,\" first_max: \",n,\" len_max: \",len_max)\n",
    "    len_dif = len(seq_test[event_id][2]) - n\n",
    "\n",
    "    # if len_max < p_steps:\n",
    "    #     peak_horizon_steps_calc = len_max - peak_horizon_steps\n",
    "        # peak_horizon_steps_calc = max(peak_horizon_steps_calc, 0)\n",
    "    # else:\n",
    "    peak_horizon_steps_calc = peak_horizon_steps\n",
    "    # n = np.argmax(seq_test[event_id][2][:, 0, 0])\n",
    "    n = n + peak_horizon_steps_calc\n",
    "    n = n.astype(int)\n",
    "    # if max(seq_test[event_id][2][:, 0, 0]) > 0:\n",
    "        # n = -12\n",
    "    # if n <= len(seq_test[event_id][2]):\n",
    "    #     n = 0\n",
    "    \n",
    "    n= max(n, 0)\n",
    "    n = min(n, len(seq_test[event_id][2])-1)\n",
    "    # n = 11\n",
    "    dict['n'] = n\n",
    "    # print(\"event: \",event_id, \" max_q: \",max_q,\" n: \",n,\" len_max: \",len_max, \" first max: \", first_max)\n",
    "    # print(\"index: \",n)\n",
    "    # print(\"len event: \", len(seq_test[event_id][2]))\n",
    "    actual_seq = seq_test[event_id][2][n]\n",
    "    dict['actual_seq'], dict['duration_col'], dict['start_time'], dict['end_time'] = add_dur(actual_seq,event_id, n)\n",
    "    # Dictionary of event meta data\n",
    "    dict['meta'] = seq_test[event_id][0] #'name': sample_name, 'duration': event_duration, 'total precipitation': precip_sum, 'max intensity': max_intensity, 'interval': intervall, 'Ereignis': type\n",
    "\n",
    "    i = 0\n",
    "    for m in models:\n",
    "        seq_test = m['seq_test']\n",
    "        seq_test_trans = m['seq_test_trans']\n",
    "        model = m['model']\n",
    "\n",
    "        interval = seq_test[event_id][0]['interval']\n",
    "        meta = seq_test[event_id][0]\n",
    "        in_seq = seq_test[event_id][1][n]\n",
    "        out_act = seq_test[event_id][2][n]\n",
    "\n",
    "        # Predict the sequence\n",
    "        Predict = model.predict(seq_test_trans[event_id][1], verbose=0)\n",
    "        Predict_invert = out_scaler.inverse_transform(Predict)\n",
    "\n",
    "        predict_seq = np.column_stack((dict['duration_col'],Predict_invert[n]))\n",
    "        dict[m['print_name']] = predict_seq\n",
    "\n",
    "        # Calculate the peak metrics time dependent\n",
    "        argmax = np.argmax(dict['actual_seq'][:, 1])\n",
    "        actual_max = dict['actual_seq'][argmax, 1]\n",
    "        predict_max = predict_seq[argmax, 1]\n",
    "        extreme_abs_error = abs(actual_max - predict_max)\n",
    "        extreme_squared_error = extreme_abs_error**2\n",
    "        sqrt_errors[i] = np.append(sqrt_errors[i], extreme_squared_error)\n",
    "        abs_errors[i] = np.append(abs_errors[i], extreme_abs_error)\n",
    "        epsilon = 0\n",
    "        ape =extreme_abs_error / (actual_max + epsilon) * 100\n",
    "        if np.isinf(ape) == False:\n",
    "            abs_prct_errors[i] = np.append(abs_prct_errors[i], ape)\n",
    "\n",
    "        # Calculate the peak metrics time independent\n",
    "        argmax = np.argmax(dict['actual_seq'][:, 1])\n",
    "        actual_max = dict['actual_seq'][argmax, 1]\n",
    "        pred_argmax = np.argmax(predict_seq[:, 1])\n",
    "        predict_max = predict_seq[pred_argmax, 1]\n",
    "        extreme_abs_error = abs(actual_max - predict_max)\n",
    "        extreme_squared_error = extreme_abs_error**2\n",
    "        sqrt_errors_T[i] = np.append(sqrt_errors_T[i], extreme_squared_error)\n",
    "        abs_errors_T[i] = np.append(abs_errors_T[i], extreme_abs_error)\n",
    "        epsilon = 0\n",
    "        ape =extreme_abs_error / (actual_max + epsilon) * 100\n",
    "        if np.isinf(ape) == False:\n",
    "            abs_prct_errors_T[i] = np.append(abs_prct_errors_T[i], ape)\n",
    "        abs_dT[i] = np.append(abs_dT[i],abs(argmax - pred_argmax) * interval)\n",
    "        i+=1\n",
    "\n",
    "    event_dicts.append(dict)\n",
    "    # else:\n",
    "    #     event_dicts.append(None)\n",
    "    \n",
    "\n",
    "peak_maes = []\n",
    "peak_rmses = []\n",
    "peak_mapes = []\n",
    "peak_maes_T = []\n",
    "peak_rmses_T = []\n",
    "peak_mapes_T = []\n",
    "peak_dT = []\n",
    "\n",
    "for i in range(len(abs_errors)):\n",
    "    mae = np.mean(abs_errors[i])\n",
    "    rmse = np.sqrt(np.mean(sqrt_errors[i]))\n",
    "    ape = abs_prct_errors[i][~np.isnan(abs_prct_errors[i])]\n",
    "    mape = np.mean(ape)\n",
    "    peak_maes.append(round(mae,3))\n",
    "    peak_rmses.append(round(rmse,3))\n",
    "    peak_mapes.append(round(mape,1))\n",
    "\n",
    "    mae_T = np.mean(abs_errors_T[i])\n",
    "    rmse_T = np.sqrt(np.mean(sqrt_errors_T[i]))\n",
    "    ape_T = abs_prct_errors_T[i][~np.isnan(abs_prct_errors_T[i])]\n",
    "    mape_T = np.mean(ape_T)\n",
    "    peak_maes_T.append(round(mae_T,3))\n",
    "    peak_rmses_T.append(round(rmse_T,3))\n",
    "    peak_mapes_T.append(round(mape_T,1))\n",
    "    peak_dT.append(round(np.mean(abs_dT[i]),2))\n",
    "\n",
    "peak_eval = pd.DataFrame({\n",
    "    'Modell': [models[i]['print_name'] for i in range(len(models))],\n",
    "    'MAE [' + out_unit + ']': peak_maes,\n",
    "    'RMSE [' + out_unit + ']': peak_rmses,\n",
    "    'MAPE [%]': peak_mapes\n",
    "})\n",
    "\n",
    "peak_eval_dT = pd.DataFrame({\n",
    "    'Modell': [models[i]['print_name'] for i in range(len(models))],\n",
    "    'MAEmax [' + out_unit + ']': peak_maes_T,\n",
    "    'RMSEmax [' + out_unit + ']': peak_rmses_T,\n",
    "    'MAPEmax [%]': peak_mapes_T,\n",
    "    'dt [min]': peak_dT\n",
    "})\n",
    "display(Markdown('**Auswertung der Maximalwertabweichungen**'))\n",
    "print(tabulate(peak_eval_dT, headers='keys', tablefmt='psql', showindex=False))\n",
    "\n",
    "# display(Markdown('<br>**Auswertung der Maximalwertabweichungen zu gleicher Zeit**'))\n",
    "\n",
    "# print(tabulate(peak_eval, headers='keys', tablefmt='psql', showindex=False))\n",
    "\n",
    "display(Markdown('<br>**Darstellung aller Maximalwertvorhersagen**'))\n",
    "\n",
    "plot_rows = int(len(seq_test_trans)/2+1)\n",
    "fig, axs = plt.subplots(plot_rows, 2, figsize=(10, 1.5*len(seq_test_trans)))\n",
    "# Plot the peaks of the events\n",
    "for event_id, dict in zip(range(len(seq_test_trans)), event_dicts):\n",
    "\n",
    "    if dict is not None:\n",
    "        col_index = event_id % 2\n",
    "        n = dict['n']\n",
    "        meta = dict['meta']\n",
    "\n",
    "        # Create Barplot of Precipitation\n",
    "        ax1 = axs[event_id // 2, col_index]\n",
    "        ax1.set_title(f\"Ereignis: {meta['event type']}, {round(meta['total precipitation'],1)} mm, {meta['duration']} min\", pad=20)\n",
    "        x = seq_test[event_id][1][n][:,0]  # Set x-axis values\n",
    "        ax1.bar(x, seq_test[event_id][1][n][:,1], color='lightblue', label='iN', width=interval, align='edge')\n",
    "        top_lim = max(max(seq_test[event_id][1][n][:,1]), 50)\n",
    "        ax1.set_ylim(bottom=0, top=top_lim)  # Set y-axis to start from zero\n",
    "        ax1.set_ylabel('Niederschlagsintensität iN [mm/h]')\n",
    "        ax1.set_xlabel('Ereignisdauer [min]')\n",
    "        # ax1.legend(fontsize = 'small')\n",
    "        # Create a twin axis on the right side\n",
    "        # Plotting the predicted and actual values in the corresponding subplot\n",
    "        ax2 = ax1.twinx()\n",
    "        blue_patch = [mpatches.Patch(color='lightblue', label='iN')]\n",
    "        for m in models:\n",
    "            ax2.plot(dict[m['print_name']][:, 0], dict[m['print_name']][:, 1], label=m['print_name'])\n",
    "        ax2.plot(dict['actual_seq'][:, 0], dict['actual_seq'][:, 1], color = 'black', label='Wahr', alpha=0.70)\n",
    "        ax2.set_ylim(bottom=0)  # Set y-axis to start from zero\n",
    "        ax2.set_ylabel('Abfluss Q [' + out_unit + ']')\n",
    "        plt.legend(fontsize = 'small')\n",
    "        old_handles, labels = ax2.get_legend_handles_labels()\n",
    "        plt.legend(handles=old_handles + blue_patch, fontsize = 'small', frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8.792864,
   "end_time": "2024-06-10T20:15:23.396620",
   "environment_variables": {},
   "exception": true,
   "input_path": "C:\\Users\\fl-al\\PythonProjects\\urbanml\\model_testing.ipynb",
   "output_path": "C:\\Users\\fl-al\\PythonProjects\\urbanml\\07_model_compare\\Eval_Test_2024-06-10.ipynb",
   "parameters": {
    "base_folder": "05_models\\train_test",
    "model_alias": [
     "\"Test Model\""
    ],
    "model_names": [
     "Gievenbeck_LSTM_Test"
    ],
    "title": "Test evaluation"
   },
   "start_time": "2024-06-10T20:15:14.603756",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}